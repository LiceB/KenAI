{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import face_recognition as fr\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo pré-treinado\n",
    "model_save_path = r'D:\\FIAP\\2024\\1_Semestre\\AI ENGENEERING, COGNITIVE AND SEMANTIC COMPUTATION & IOT\\CP\\KenAI\\Reconhecimento_pessoa\\modelo2.h5'\n",
    "modelo = tf.keras.models.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar a captura de vídeo do arquivo\n",
    "cap = cv2.VideoCapture('videos/Larissa.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: List[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x000001A3EF74F9B0>, array([[[161, 131,  87],\n        [158, 128,  84],\n        [158, 128,  84],\n        ...,\n        [ 83,  66,  38],\n        [149, 125,  90],\n        [152, 129,  87]],\n\n       [[161, 131,  87],\n        [158, 128,  84],\n        [157, 127,  83],\n        ...,\n        [ 85,  68,  40],\n        [150, 126,  91],\n        [153, 130,  88]],\n\n       [[160, 130,  84],\n        [158, 128,  82],\n        [158, 128,  82],\n        ...,\n        [114,  98,  67],\n        [150, 126,  89],\n        [153, 130,  88]],\n\n       ...,\n\n       [[208, 186, 194],\n        [195, 173, 181],\n        [190, 168, 173],\n        ...,\n        [150, 126, 132],\n        [142, 120, 123],\n        [124, 105, 109]],\n\n       [[206, 187, 191],\n        [188, 169, 173],\n        [200, 176, 182],\n        ...,\n        [149, 132, 135],\n        [123,  99, 105],\n        [126, 102, 108]],\n\n       [[206, 187, 191],\n        [172, 153, 157],\n        [202, 178, 184],\n        ...,\n        [146, 129, 132],\n        [123,  99, 105],\n        [124, 100, 106]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x000001A3F89A25F0>, 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Chamar a função de reconhecimento facial no vídeo\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[43mvideo_face_recognition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcap\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36mvideo_face_recognition\u001b[1;34m(model, cap)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (top, right, bottom, left) \u001b[38;5;129;01min\u001b[39;00m face_locations:\n\u001b[0;32m     18\u001b[0m     face_img \u001b[38;5;241m=\u001b[39m rgb_frame[top:bottom, left:right]  \u001b[38;5;66;03m# Recortar o rosto\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     face_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mfr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_img\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Obter o encoding do rosto\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m face_encodings:\n\u001b[0;32m     21\u001b[0m         face_encoding \u001b[38;5;241m=\u001b[39m face_encodings[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[38;5;241m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_face_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_landmark_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_jitters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_landmark_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_landmarks\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[38;5;241m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(\u001b[43mface_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_face_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_landmark_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_jitters\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: List[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: List[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x000001A3EF74F9B0>, array([[[161, 131,  87],\n        [158, 128,  84],\n        [158, 128,  84],\n        ...,\n        [ 83,  66,  38],\n        [149, 125,  90],\n        [152, 129,  87]],\n\n       [[161, 131,  87],\n        [158, 128,  84],\n        [157, 127,  83],\n        ...,\n        [ 85,  68,  40],\n        [150, 126,  91],\n        [153, 130,  88]],\n\n       [[160, 130,  84],\n        [158, 128,  82],\n        [158, 128,  82],\n        ...,\n        [114,  98,  67],\n        [150, 126,  89],\n        [153, 130,  88]],\n\n       ...,\n\n       [[208, 186, 194],\n        [195, 173, 181],\n        [190, 168, 173],\n        ...,\n        [150, 126, 132],\n        [142, 120, 123],\n        [124, 105, 109]],\n\n       [[206, 187, 191],\n        [188, 169, 173],\n        [200, 176, 182],\n        ...,\n        [149, 132, 135],\n        [123,  99, 105],\n        [126, 102, 108]],\n\n       [[206, 187, 191],\n        [172, 153, 157],\n        [202, 178, 184],\n        ...,\n        [146, 129, 132],\n        [123,  99, 105],\n        [124, 100, 106]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x000001A3F89A25F0>, 1"
     ]
    }
   ],
   "source": [
    "def video_face_recognition(model, cap):\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()  # Captura um frame do vídeo\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Redimensionar o frame \n",
    "        resized_frame = cv2.resize(frame, (0, 0), fx=0.2, fy=0.2)\n",
    "\n",
    "        # Converter o frame para RGB para o face_recognition\n",
    "        rgb_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Encontrar as localizações dos rostos no frame usando face_recognition\n",
    "        face_locations = fr.face_locations(rgb_frame)\n",
    "\n",
    "        # Pré-processar cada rosto e calcular os face encodings\n",
    "        for (top, right, bottom, left) in face_locations:\n",
    "            face_img = rgb_frame[top:bottom, left:right]  # Recortar o rosto\n",
    "            face_encodings = fr.face_encodings(face_img)  # Obter o encoding do rosto\n",
    "            if face_encodings:\n",
    "                face_encoding = face_encodings[0]\n",
    "\n",
    "                # Adicionar dimensão de lote\n",
    "                face_encoding = np.expand_dims(face_encoding, axis=0)\n",
    "\n",
    "                # Fazer previsão usando o modelo\n",
    "                predictions = model.predict(face_encoding)\n",
    "                predicted_class = np.argmax(predictions)\n",
    "\n",
    "                # Definir a cor e o rótulo com base na classe prevista\n",
    "                color = (0, 255, 0) if predicted_class == 0 else (0, 0, 255)  # Verde para reconhecido, vermelho para não reconhecido\n",
    "                label = \"Larissa\" if predicted_class == 0 else \"Outra pessoa\"\n",
    "\n",
    "                # Desenhar o retângulo ao redor do rosto detectado no vídeo e escrever o nome\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
    "                cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "\n",
    "        # Mostrar o frame com o resultado do reconhecimento facial\n",
    "        cv2.imshow('Video Face Recognition', frame)\n",
    "\n",
    "        # Pressione 'esc' para sair do loop\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    # Liberar o vídeo e fechar a janela\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Chamar a função de reconhecimento facial no vídeo\n",
    "video_face_recognition(modelo, cap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
